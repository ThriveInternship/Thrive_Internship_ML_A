# -*- coding: utf-8 -*-
"""03_finetune_distilBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_I-lJDPhviBE-JqJHtw88_meWtEBH75s

Installation of Tranform Library
"""

!pip install -q transformers datasets torch scikit-learn pandas

"""# DistilBERT Customer Support Classification Model
A model trained to classify customer messages into predefined categories which includes accounts, billing, other and technical.

What this Model can be used for:


*  Ticket Routing: Automatically directing messages to the right department or agent (eg. teachnical team)

*  Response Prioritisation: Identify urgent or negative messages that needs quick attention.
*   Customer Analytics: Understand trends in complaints, feedback or issues over time.


*   Quality Monitoring: Classify messages for sentiment or compliance checks.


"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import torch
import pandas as pd
import numpy as np
import os

!pip install -q mlflow

import mlflow
import os

# Set experiment name
mlflow.set_experiment("baseline_models")

# Define safe run starter to handle nested runs cleanly
def safe_start_run(run_name, nested=False):
    if mlflow.active_run() is not None:
        mlflow.end_run()
    return mlflow.start_run(run_name=run_name, nested=nested)

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/clean_tickets.csv"
df = pd.read_csv(path)

print("Dataset overview:")
print(f"Total samples: {len(df)}")
print(f"\nLabels distribution:\n{df['label'].value_counts()}")
print(f"\nAgents distribution:\n{df['agent_name'].value_counts()}")

import pandas as pd
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset
from sentence_transformers import SentenceTransformer, util
import torch

# ==========================
# 1. Prepare the dataframe
# ==========================
df = df[['text', 'label']].copy()

# Encode labels (convert class names to numbers)
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['label'])

print(f"Initial samples: {len(df)}")

# ======================================================
# 2. Create semantic groups to prevent data leakage
# ======================================================
# Compute sentence embeddings using a lightweight transformer
model = SentenceTransformer("all-MiniLM-L6-v2")

print("üîç Computing embeddings for semantic grouping...")
embeddings = model.encode(df['text'].tolist(), convert_to_tensor=True, show_progress_bar=True)

# Compute pairwise cosine similarity
cosine_sim = util.cos_sim(embeddings, embeddings)

# Cluster near-duplicates: if cosine similarity > threshold, they share a group
threshold = 0.9
group_ids = [-1] * len(df)
current_group = 0

for i in range(len(df)):
    if group_ids[i] != -1:
        continue
    group_ids[i] = current_group
    sims = cosine_sim[i]
    near_dups = torch.where(sims > threshold)[0].tolist()
    for j in near_dups:
        group_ids[j] = current_group
    current_group += 1

df['group'] = group_ids
print(f"‚úÖ Created {len(set(group_ids))} semantic groups from {len(df)} samples.")

# ======================================================
# 3. Split using StratifiedGroupKFold (80/10/10)
# ======================================================
sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)

# Take the first fold for test set
for train_val_idx, test_idx in sgkf.split(df, df['label'], groups=df['group']):
    train_val_df = df.iloc[train_val_idx]
    test_df = df.iloc[test_idx]
    break

# Now split train_val into train/val
sgkf_inner = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)
for train_idx, val_idx in sgkf_inner.split(train_val_df, train_val_df['label'], groups=train_val_df['group']):
    train_df = train_val_df.iloc[train_idx]
    val_df = train_val_df.iloc[val_idx]
    break

print(f"Train size: {len(train_df)} | Val size: {len(val_df)} | Test size: {len(test_df)}")
print(f"Unique groups: Train={train_df['group'].nunique()}, Val={val_df['group'].nunique()}, Test={test_df['group'].nunique()}")

# ======================================================
# 4. Check for data leakage between splits
# ======================================================
def check_data_leakage(train_df, test_df):
    overlap = set(train_df['text']).intersection(set(test_df['text']))
    print(f"Overlap count: {len(overlap)}")
    if len(overlap) > 0:
        print("‚ö†Ô∏è Data leakage detected!")
    else:
        print("‚úÖ No overlap ‚Äî group-aware split is clean.")

check_data_leakage(train_df, val_df)
check_data_leakage(train_df, test_df)
check_data_leakage(val_df, test_df)

# ======================================================
# 5. Convert to Hugging Face Dataset format
# ======================================================
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)
test_dataset = Dataset.from_pandas(test_df)

print("‚úÖ All datasets ready for DistilBERT fine-tuning!")

"""# Essential Tools for the DistilBERT Model

Tools Include:
*   Tokenizer: Convert text (eg messages) to numbers (eg o's and 1's).
*   AutoTokenizer: Automatically picks the correct tokenizer for the pretrained DistilBERT.


"""

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize(batch):
    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# Set format for PyTorch
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

num_labels = len(df['label'].unique())

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=num_labels
)

# ==========================================
# Compute Class Weights (to handle imbalance)
# ==========================================
class_counts = train_df['label'].value_counts().sort_index()
print("Class counts:\n", class_counts)

# Inverse frequency weighting
class_weights = 1.0 / class_counts
class_weights = class_weights / class_weights.sum() * len(class_counts)
class_weights = torch.tensor(class_weights.values, dtype=torch.float)

# Move weights to device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
class_weights = class_weights.to(device)

print("Computed class weights:", class_weights)

# ==========================================
# Compute Class Weights (to handle imbalance)
# ==========================================
class_counts = train_df['label'].value_counts().sort_index()
print("Class counts:\n", class_counts)

# Inverse frequency weighting
class_weights = 1.0 / class_counts
class_weights = class_weights / class_weights.sum() * len(class_counts)
class_weights = torch.tensor(class_weights.values, dtype=torch.float)

# Move weights to device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
class_weights = class_weights.to(device)

print("Computed class weights:", class_weights)

# ==========================================
# Custom Trainer with Weighted Loss
# ==========================================
import torch.nn as nn # Import the nn module

class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")

        # Apply weighted CrossEntropyLoss
        loss_fct = nn.CrossEntropyLoss(weight=class_weights)
        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))

        return (loss, outputs) if return_outputs else loss

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,    # Train for 3‚Äì5 epochs
    weight_decay=0.01,
    load_best_model_at_end=True,
    logging_dir="./logs",
    report_to="none" # Explicitly disable reporting to wandb
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
)

os.environ["WANDB_DISABLED"] = "true"
trainer.train()

# Evaluate on the test dataset after training
trainer.evaluate(test_dataset)

"""os.environ["WANDB_DISABLED"] = "true"
trainer.train()

trainer.evaluate(test_dataset)

# Model Classification Report
Here processes and activity such as:

*   Manual defining of labels to encoders
*   Evaluation of Model classification performance using metrics such as precision, recall, f1-score and support.


*   Generation of a confusion matrix to visualise the accuracy between the actual and predicted.
"""

# Manually define the mapping
label_map = {
    0: "account",
    1: "billing",
    2: "other",
    3: "technical"
}

# Get predictions from the trainer
# Use the standard Trainer for prediction
standard_trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, eval_dataset=test_dataset)
predictions = standard_trainer.predict(test_dataset)
true_labels = predictions.label_ids
pred_labels = predictions.predictions.argmax(-1)


# Decode numeric labels using the map
decoded_true = [label_map[i] for i in true_labels]
decoded_pred = [label_map[i] for i in pred_labels]

# Print mapping
print("===== LABEL ENCODER MAPPING =====")
for k, v in label_map.items():
    print(f"{k} ‚Üí {v}")

# Classification report
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd

print("\n===== CLASSIFICATION REPORT =====")
report = classification_report(decoded_true, decoded_pred, target_names=list(label_map.values()))
print(report)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix


# 1Ô∏è‚É£ Compute the confusion matrix
cm = confusion_matrix(decoded_true, decoded_pred, labels=list(label_map.values()))
cm_df = pd.DataFrame(cm, index=list(label_map.values()), columns=list(label_map.values()))
print("\n===== CONFUSION MATRIX =====")
print(cm_df)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=True, linewidths=0.5)
plt.title("Confusion Matrix Heatmap", fontsize=14, pad=15)
plt.xlabel("Predicted Labels", fontsize=12)
plt.ylabel("True Labels", fontsize=12)
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""**Comment On Performance: **
*   Accuracy	0.93	The model correctly predicted 93% of all test tickets ‚Äî solid generalization.

*  Macro avg F1	0.93	Balanced average of all classes ‚Äî shows consistent performance across categories.
* Weighted avg F1	0.93	Takes into account class size ‚Äî confirms strong, balanced performance overall.

# Checking for Data Leakeage
Inspecting if the model leant the patterns in the dataset in the train split  or just copy and pasted datasets in the test split.
"""

set(train_df['text']).intersection(set(test_df['text']))

"""# Verification of Label Encoder Mapping"""

from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Example: assume your dataset column for labels is 'rock_type'
y = df['label']

# Initialize and fit encoder
label_encoder = LabelEncoder()
encoded_y = label_encoder.fit_transform(y)

# Get mapping
class_names = label_encoder.classes_

# Show what each encoded number represents
print("===== VERIFIED LABEL ENCODER MAPPING =====")
for i, name in enumerate(class_names):
    print(f"{i} ‚Üí {name}")

# (Optional) Check sample side-by-side to confirm match
mapping_check = pd.DataFrame({
    'Original_Label': y,
    'Encoded_Value': encoded_y
}).drop_duplicates().sort_values('Encoded_Value')

print("\n===== SAMPLE CHECK =====")
print(mapping_check)

"""ML Flow Integration"""

# ============================
# MLflow tracking integration
# ============================

# Start a parent MLflow run for DistilBERT fine-tuning
with safe_start_run("DistilBERT Fine-tuning") as parent_run:
    # Example: log training parameters (you can add more)
    mlflow.log_param("model_name", "distilbert-base-uncased")
    mlflow.log_param("epochs", training_args.num_train_epochs)
    mlflow.log_param("learning_rate", training_args.learning_rate)
    mlflow.log_param("batch_size", training_args.per_device_train_batch_size)

    # After model training and evaluation, log metrics
    # (Assuming 'metrics' dictionary exists after evaluation)
    if 'metrics' in locals():
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                mlflow.log_metric(key, value)

    # Optionally log confusion matrix or reports if available
    if os.path.exists("confusion_matrix.png"):
        mlflow.log_artifact("confusion_matrix.png")

    if os.path.exists("classification_report.txt"):
        mlflow.log_artifact("classification_report.txt")

    print("MLflow logging completed for DistilBERT fine-tuning.")

model.save_pretrained("./03_finetune_distilBERT")
tokenizer.save_pretrained("./03_finetune_distilBERT")

#!zip -r 03_finetune_distilBERT_model.zip /content/03_finetune_distilBERT