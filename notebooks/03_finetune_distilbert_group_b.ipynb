{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da452dcb",
   "metadata": {},
   "source": [
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4930b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HUMAIDU\\anaconda3\\envs\\mlpython310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 03_finetune_distilbert.ipynb\n",
    "# ---------------------------------\n",
    "# Week 3: DistilBERT Fine-tuning \n",
    "# Purpose: fine-tune distilbert-base-uncased on the cleaned ticket dataset, evaluate, save artifacts,\n",
    "# and log everything to MLflow (with safety checks for active runs).\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import transformers\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528b8ef",
   "metadata": {},
   "source": [
    "# Paths and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e027f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x241f112bab0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLEANED_DATA_PATH = \"../data/file-for-fineTuning.csv\" \n",
    "MODEL_DIR = \"../models/distilbert-ticket-classifier\"\n",
    "ARTIFACTS_DIR = \"../artifacts\"\n",
    "RESULTS_DIR = \"../results\"\n",
    "MLFLOW_TRACKING_URI = \"file:../mlruns\"\n",
    "EXPERIMENT_NAME = \"transformer_finetuning_experiment\"\n",
    "\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 128\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7787ce77",
   "metadata": {},
   "source": [
    "#### Cell 3: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279a93d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned dataset: ../data/file-for-fineTuning.csv\n",
      "Classes: {'account': 0, 'billing': 1, 'other': 2, 'tech_support': 3}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading cleaned dataset:\", CLEANED_DATA_PATH)\n",
    "df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Update these names if your columns differ\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "\n",
    "if TEXT_COL not in df.columns or LABEL_COL not in df.columns:\n",
    "    print(\"Dataset columns:\", df.columns.tolist())\n",
    "    raise ValueError(f\"Expected columns '{TEXT_COL}' and '{LABEL_COL}' in the cleaned CSV\")\n",
    "\n",
    "\n",
    "# Convert labels to integer ids and keep mapping\n",
    "df[LABEL_COL] = df[LABEL_COL].astype('category')\n",
    "label2id = {c: i for i, c in enumerate(df[LABEL_COL].cat.categories)}\n",
    "id2label = {i: c for c, i in label2id.items()}\n",
    "df['label_id'] = df[LABEL_COL].cat.codes\n",
    "\n",
    "\n",
    "print(\"Classes:\", label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d65787",
   "metadata": {},
   "source": [
    "#### Train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111640c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 396, Val size: 99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "texts = df[TEXT_COL].astype(str).tolist()\n",
    "labels = df['label_id'].tolist()\n",
    "\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "texts, labels, test_size=0.2, stratify=labels, random_state=SEED\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Train size: {len(train_texts)}, Val size: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e2872",
   "metadata": {},
   "source": [
    "### Tokenizer & encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dbd9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "DistilBertTokenizerFast,\n",
    "DistilBertForSequenceClassification,\n",
    "TrainingArguments,\n",
    ")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d87dbd",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58192e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicketDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "\n",
    "train_dataset = TicketDataset(train_encodings, train_labels)\n",
    "val_dataset = TicketDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5834be",
   "metadata": {},
   "source": [
    "### Model & TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f77246dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers working!\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "print(\"Transformers working!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfad63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7420b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label2id)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trainer_results\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d37bc2",
   "metadata": {},
   "source": [
    "### Metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aad4e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_weighted': f1,\n",
    "        'precision_weighted': precision,\n",
    "        'recall_weighted': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a2bd9",
   "metadata": {},
   "source": [
    "### Trainer & Training (with MLflow logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc46ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HUMAIDU\\AppData\\Local\\Temp\\ipykernel_2352\\1897967939.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\HUMAIDU\\anaconda3\\envs\\mlpython310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025/11/06 12:06:59 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id 7e2db463f34e4f9a865afbf26f97a8a5: Failed to log run data: Exception: Changing param values is not allowed. Param with key='max_length' was already logged with value='128' for run ID='7e2db463f34e4f9a865afbf26f97a8a5'. Attempted logging new value '20'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 01:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.111200</td>\n",
       "      <td>0.724400</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.853244</td>\n",
       "      <td>0.857326</td>\n",
       "      <td>0.858586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.565500</td>\n",
       "      <td>0.372956</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.938692</td>\n",
       "      <td>0.947975</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.313393</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.938692</td>\n",
       "      <td>0.947975</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HUMAIDU\\anaconda3\\envs\\mlpython310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\HUMAIDU\\anaconda3\\envs\\mlpython310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\HUMAIDU\\anaconda3\\envs\\mlpython310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval result: {'eval_loss': 0.3133927583694458, 'eval_accuracy': 0.9393939393939394, 'eval_f1_weighted': 0.9386916786916787, 'eval_precision_weighted': 0.9479747305834262, 'eval_recall_weighted': 0.9393939393939394, 'eval_runtime': 1.8063, 'eval_samples_per_second': 54.809, 'eval_steps_per_second': 7.197, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HUMAIDU\\anaconda3\\envs\\mlpython310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and MLflow logging completed. Model saved to: ../models/distilbert-ticket-classifier\n",
      "Metrics saved to: ../artifacts\\transformer_metrics.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "#To ensure no active MLflow run exists before starting a new one\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "run_name = f\"DistilBERT_Run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    # Log basic params\n",
    "    mlflow.log_param('model', 'distilbert-base-uncased')\n",
    "    mlflow.log_param('num_epochs', NUM_EPOCHS)\n",
    "    mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "    mlflow.log_param('max_length', MAX_LENGTH)\n",
    "    mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model(MODEL_DIR)\n",
    "    tokenizer.save_pretrained(MODEL_DIR)\n",
    "\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    eval_result = trainer.evaluate(eval_dataset=val_dataset)\n",
    "    print(\"Eval result:\", eval_result)\n",
    "\n",
    "\n",
    "    # Predictions for metrics and confusion matrix\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = predictions.label_ids\n",
    "\n",
    "\n",
    "    # Compute sklearn metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_weighted': float(f1),\n",
    "        'precision_weighted': float(precision),\n",
    "        'recall_weighted': float(recall),\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label,\n",
    "        'train_size': len(train_dataset),\n",
    "        'val_size': len(val_dataset)\n",
    "    }\n",
    "\n",
    "\n",
    "    # Save metrics to artifacts\n",
    "    metrics_path = os.path.join(ARTIFACTS_DIR, 'transformer_metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "    # Log metrics & artifacts to MLflow\n",
    "    mlflow.log_metrics({k: v for k, v in metrics.items() if isinstance(v, (int, float))})\n",
    "    mlflow.log_artifacts(ARTIFACTS_DIR)\n",
    "    mlflow.log_artifacts(MODEL_DIR)\n",
    "\n",
    "    # Save predictions for later analysis\n",
    "    preds_df = pd.DataFrame({\n",
    "        'text': val_texts,\n",
    "        'y_true': [id2label[i] for i in y_true],\n",
    "        'y_pred': [id2label[i] for i in y_pred]\n",
    "    })\n",
    "    preds_df.to_csv(os.path.join(ARTIFACTS_DIR, 'transformer_val_predictions.csv'), index=False)\n",
    "    mlflow.log_artifact(os.path.join(ARTIFACTS_DIR, 'transformer_val_predictions.csv'))\n",
    "\n",
    "\n",
    "print(\"Training and MLflow logging completed. Model saved to:\", MODEL_DIR)\n",
    "print(\"Metrics saved to:\", metrics_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8212ee5",
   "metadata": {},
   "source": [
    "### Quick evaluation plots (confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_text = (\n",
    "    f\"DistilBERT fine-tuning report\\n\"\n",
    "    f\"===============================\\n\"\n",
    "    f\"Train size: {len(train_dataset)}\\n\"\n",
    "    f\"Val size: {len(val_dataset)}\\n\"\n",
    "    f\"Accuracy: {metrics['accuracy']:.4f}\\n\"\n",
    "    f\"F1 (weighted): {metrics['f1_weighted']:.4f}\\n\"\n",
    "    f\"Precision (weighted): {metrics['precision_weighted']:.4f}\\n\"\n",
    "    f\"Recall (weighted): {metrics['recall_weighted']:.4f}\\n\"\n",
    ")\n",
    "with open(os.path.join(RESULTS_DIR, 'distilbert_evaluation_report.txt'), 'w') as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "\n",
    "print('Evaluation report saved to', os.path.join(RESULTS_DIR, 'distilbert_evaluation_report.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpython310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
