{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9b3c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ================== Configuration ==================\n",
    "class Config:\n",
    "    DATA_PATH = \"customer_support_tickets_clean_500 (3).csv\"\n",
    "    MODEL_NAME = \"distilbert-base-uncased\"\n",
    "    SAVE_DIR = \"models/distilbert-ticket-classifier\"\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 2e-5\n",
    "    TEST_SIZE = 0.2\n",
    "    RANDOM_STATE = 42\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ================== Dataset Class ==================\n",
    "class TicketDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96a8c5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset...\n",
      "‚úì Loaded 495 tickets\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "Other        125\n",
      "Technical    119\n",
      "Account      115\n",
      "Billing      113\n",
      "unknown       23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping: {'Account': 0, 'Billing': 1, 'Other': 2, 'Technical': 3, 'unknown': 4}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================== Data Loading & Preprocessing ==================\n",
    "def load_and_preprocess_data(config):\n",
    "    print(\"üìÇ Loading dataset...\")\n",
    "    df = pd.read_csv(config.DATA_PATH)\n",
    "    \n",
    "    # Remove rows with 'unknown' in text column (if they exist)\n",
    "    df = df[df['text'] != 'unknown'].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úì Loaded {len(df)} tickets\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_list = sorted(df['label'].unique())\n",
    "    label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    \n",
    "    print(f\"\\nLabel mapping: {label2id}\")\n",
    "    df['label'] = df['label'].map(label2id)\n",
    "    df['label_id'] = df['label'].map(label2id)\n",
    "    return df, label2id, id2label\n",
    "\n",
    "\n",
    "class Config:\n",
    "    DATA_PATH = \"customer_support_tickets_clean_500 (3).csv\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "df, label2id, id2label = load_and_preprocess_data(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb37fc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì label_id column populated\n"
     ]
    }
   ],
   "source": [
    " # Convert labels to IDs\n",
    "# If labels are strings, map them to ids; if they're already integers, leave as is.\n",
    "if df['label'].dtype == object:\n",
    "\tdf['label'] = df['label'].map(label2id)\n",
    "\n",
    "# Ensure label_id column exists and contains integer label ids\n",
    "df['label_id'] = df['label'].astype(int)\n",
    "\n",
    "\n",
    "print(\"‚úì label_id column populated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a98d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(df, label2id, id2label, config):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets based on configuration.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The preprocessed dataframe containing 'text' and 'label_id'.\n",
    "        label2id (dict): Mapping from label string to label id.\n",
    "        id2label (dict): Mapping from label id to label string.\n",
    "        config (object): Configuration object with TEST_SIZE and RANDOM_STATE attributes.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: train_texts, test_texts, train_labels, test_labels, label2id, id2label\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Validation checks ---\n",
    "    if not hasattr(config, \"TEST_SIZE\") or not hasattr(config, \"RANDOM_STATE\"):\n",
    "        raise AttributeError(\"‚ùå Config object must define TEST_SIZE and RANDOM_STATE.\")\n",
    "\n",
    "    if \"text\" not in df.columns or \"label_id\" not in df.columns:\n",
    "        raise KeyError(\"‚ùå DataFrame must contain 'text' and 'label_id' columns.\")\n",
    "\n",
    "    # --- Split dataset ---\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        df[\"text\"].values,\n",
    "        df[\"label_id\"].values,\n",
    "        test_size=config.TEST_SIZE,\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        stratify=df[\"label_id\"].values\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úì Split completed: {len(train_texts)} train samples, {len(test_texts)} test samples\")\n",
    "\n",
    "    return train_texts, test_texts, train_labels, test_labels, label2id, id2label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351ca698",
   "metadata": {},
   "outputs": [],
   "source": [
    " #================== Model Training ==================\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793f572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Model Evaluation ==================\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='macro'\n",
    "    )\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, predictions, true_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318b5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Main Training Pipeline ==================\n",
    "def main():\n",
    "    config = Config()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üöÄ DistilBERT Ticket Classifier Training\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Device: {config.DEVICE}\")\n",
    "    print(f\"Epochs: {config.EPOCHS}\")\n",
    "    print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "    print(f\"Learning Rate: {config.LEARNING_RATE}\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    train_texts, test_texts, train_labels, test_labels, label2id, id2label = \\\n",
    "        load_and_preprocess_data(config)\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    print(\"\\nü§ñ Loading DistilBERT model...\")\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        num_labels=len(label2id)\n",
    "    )\n",
    "    model.to(config.DEVICE)\n",
    "    print(\"‚úì Model loaded successfully\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7572f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset...\n",
      "‚úì Loaded 495 samples\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "Other        125\n",
      "Technical    119\n",
      "Account      115\n",
      "Billing      113\n",
      "unknown       23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Split completed: 396 train samples, 99 test samples\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# ‚úÖ FULL DATA PREPARATION PIPELINE\n",
    "# ======================================\n",
    "\n",
    "# Import dependencies\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Config class ---\n",
    "class Config:\n",
    "    DATA_PATH = \"customer_support_tickets_clean_500 (3).csv\"\n",
    "    TEST_SIZE = 0.2\n",
    "    RANDOM_STATE = 42\n",
    "    MAX_LENGTH = 128\n",
    "    BATCH_SIZE = 8\n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 3e-5\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# --- Load and preprocess data ---\n",
    "print(\"üìÇ Loading dataset...\")\n",
    "df = pd.read_csv(config.DATA_PATH)\n",
    "df = df[df['text'] != 'unknown'].reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úì Loaded {len(df)} samples\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# --- Create label mappings ---\n",
    "label_list = sorted(df['label'].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "df['label_id'] = df['label'].map(label2id)\n",
    "\n",
    "# --- Split dataset ---\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].values,\n",
    "    df['label_id'].values,\n",
    "    test_size=config.TEST_SIZE,\n",
    "    random_state=config.RANDOM_STATE,\n",
    "    stratify=df['label_id'].values\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Split completed: {len(train_texts)} train samples, {len(test_texts)} test samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62fdea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Load pretrained DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9cf1877",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create datasets\n",
    "train_dataset = TicketDataset(train_texts, train_labels, tokenizer, config.MAX_LENGTH)\n",
    "test_dataset = TicketDataset(test_texts, test_labels, tokenizer, config.MAX_LENGTH)\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a25f0393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Create DistilBERT model for text classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label2id)  # number of unique labels\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d320e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "total_steps = len(train_loader) * config.EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87523370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä Starting Training\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 1/3\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 2/3\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 3/3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä Starting Training\")\n",
    "print(\"=\"*60)\n",
    "    \n",
    "best_f1 = 0\n",
    "training_history = []\n",
    "    \n",
    "for epoch in range(config.EPOCHS):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{config.EPOCHS}\")\n",
    "        print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb5fe29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [04:38<00:00,  5.58s/it, loss=1.05] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Training Loss: 1.0526\n",
      "‚úì Training Accuracy: 0.7601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:15<00:00,  1.19s/it]\n",
      "c:\\Users\\CKB ENT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Validation Results:\n",
      "   Loss: 0.4998\n",
      "   Accuracy: 0.9293\n",
      "   Precision: 0.7448\n",
      "   Recall: 0.7826\n",
      "   F1-Score: 0.7630\n",
      "\n",
      "üíæ New best F1! Saving model...\n"
     ]
    }
   ],
   "source": [
    " # Train\n",
    "train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, config.DEVICE\n",
    "        )\n",
    "        \n",
    "print(f\"\\n‚úì Training Loss: {train_loss:.4f}\")\n",
    "print(f\"‚úì Training Accuracy: {train_acc:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "val_loss, val_acc, val_precision, val_recall, val_f1, _, _ = evaluate(\n",
    "            model, test_loader, config.DEVICE\n",
    "        )\n",
    "        \n",
    "print(f\"\\nüìà Validation Results:\")\n",
    "print(f\"   Loss: {val_loss:.4f}\")\n",
    "print(f\"   Accuracy: {val_acc:.4f}\")\n",
    "print(f\"   Precision: {val_precision:.4f}\")\n",
    "print(f\"   Recall: {val_recall:.4f}\")\n",
    "print(f\"   F1-Score: {val_f1:.4f}\")\n",
    "        \n",
    "training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'val_precision': val_precision,\n",
    "            'val_recall': val_recall,\n",
    "            'val_f1': val_f1\n",
    "        })\n",
    "        \n",
    "        # Save best model\n",
    "if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            print(f\"\\nüíæ New best F1! Saving model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdfb17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training device: cpu\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "print(\"‚úÖ Training device:\", config.DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f190340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ Final Evaluation on Test Set\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:16<00:00,  1.23s/it]\n",
      "c:\\Users\\CKB ENT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\CKB ENT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\CKB ENT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\CKB ENT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Test Metrics:\n",
      "   Accuracy: 0.9293 (92.93%)\n",
      "   Precision: 0.7448 (74.48%)\n",
      "   Recall: 0.7826 (78.26%)\n",
      "   F1-Score: 0.7630 (76.30%)\n",
      "\n",
      "============================================================\n",
      "üìã Detailed Classification Report\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Account       0.88      0.91      0.89        23\n",
      "     Billing       1.00      1.00      1.00        22\n",
      "       Other       0.93      1.00      0.96        25\n",
      "   Technical       0.92      1.00      0.96        24\n",
      "     unknown       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.93        99\n",
      "   macro avg       0.74      0.78      0.76        99\n",
      "weighted avg       0.88      0.93      0.91        99\n",
      "\n",
      "\n",
      "üìä Confusion Matrix:\n",
      "[[21  0  2  0  0]\n",
      " [ 0 22  0  0  0]\n",
      " [ 0  0 25  0  0]\n",
      " [ 0  0  0 24  0]\n",
      " [ 3  0  0  2  0]]\n",
      "\n",
      "============================================================\n",
      "üíæ Saving Model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CKB ENT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\CKB ENT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\CKB ENT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Model saved to: models/distilbert-ticket-classifier\n",
      "‚úì Files saved:\n",
      "   - pytorch_model.bin\n",
      "   - config.json\n",
      "   - vocab.txt\n",
      "   - tokenizer files\n",
      "   - label2id.json\n",
      "   - id2label.json\n",
      "   - training_history.json\n",
      "   - metrics.json\n",
      "\n",
      "============================================================\n",
      "üéâ Training Complete!\n",
      "============================================================\n",
      "============================================================\n",
      "üöÄ DistilBERT Ticket Classifier Training\n",
      "============================================================\n",
      "Device: cpu\n",
      "Epochs: 5\n",
      "Batch Size: 16\n",
      "Learning Rate: 2e-05\n",
      "\n",
      "üìÇ Loading dataset...\n",
      "‚úì Loaded 495 tickets\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "Other        125\n",
      "Technical    119\n",
      "Account      115\n",
      "Billing      113\n",
      "unknown       23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping: {'Account': 0, 'Billing': 1, 'Other': 2, 'Technical': 3, 'unknown': 4}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLearning Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.LEARNING_RATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m train_texts, test_texts, train_labels, test_labels, label2id, id2label = \\\n\u001b[32m     15\u001b[39m     load_and_preprocess_data(config)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mü§ñ Loading DistilBERT model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 6, got 3)"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ Final Evaluation on Test Set\")\n",
    "print(\"=\"*60)\n",
    "    \n",
    "test_loss, test_acc, test_precision, test_recall, test_f1, predictions, true_labels = \\\n",
    "        evaluate(model, test_loader, config.DEVICE)\n",
    "    \n",
    "print(f\"\\nüìä Final Test Metrics:\")\n",
    "print(f\"   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   Precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
    "print(f\"   Recall: {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
    "print(f\"   F1-Score: {test_f1:.4f} ({test_f1*100:.2f}%)\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã Detailed Classification Report\")\n",
    "print(\"=\"*60)\n",
    "    \n",
    "target_names = [id2label[i] for i in range(len(id2label))]\n",
    "print(classification_report(true_labels, predictions, target_names=target_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(\"\\nüìä Confusion Matrix:\")\n",
    "print(cm)\n",
    "    \n",
    "    # Save model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ Saving Model\")\n",
    "print(\"=\"*60)\n",
    "    \n",
    "os.makedirs(config.SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "model.save_pretrained(config.SAVE_DIR)\n",
    "tokenizer.save_pretrained(config.SAVE_DIR)\n",
    "    \n",
    "    # Save label mappings\n",
    "with open(os.path.join(config.SAVE_DIR, 'label2id.json'), 'w') as f:\n",
    "        json.dump(label2id, f, indent=2)\n",
    "    \n",
    "with open(os.path.join(config.SAVE_DIR, 'id2label.json'), 'w') as f:\n",
    "        json.dump(id2label, f, indent=2)\n",
    "    \n",
    "    # Save training history\n",
    "with open(os.path.join(config.SAVE_DIR, 'training_history.json'), 'w') as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    \n",
    "    # Save final metrics\n",
    "final_metrics = {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_precision': float(test_precision),\n",
    "        'test_recall': float(test_recall),\n",
    "        'test_f1': float(test_f1),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': classification_report(\n",
    "            true_labels, predictions, target_names=target_names, output_dict=True\n",
    "        )\n",
    "    }\n",
    "    \n",
    "with open(os.path.join(config.SAVE_DIR, 'metrics.json'), 'w') as f:\n",
    "        json.dump(final_metrics, f, indent=2)\n",
    "    \n",
    "print(f\"\\n‚úì Model saved to: {config.SAVE_DIR}\")\n",
    "print(\"‚úì Files saved:\")\n",
    "print(\"   - pytorch_model.bin\")\n",
    "print(\"   - config.json\")\n",
    "print(\"   - vocab.txt\")\n",
    "print(\"   - tokenizer files\")\n",
    "print(\"   - label2id.json\")\n",
    "print(\"   - id2label.json\")\n",
    "print(\"   - training_history.json\")\n",
    "print(\"   - metrics.json\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a3d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
